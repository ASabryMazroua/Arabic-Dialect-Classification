{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3ffd66",
   "metadata": {},
   "source": [
    "### The Machine Learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "999baf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC # SVC took a very long time with very bad accuracy\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8553455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#Import the clean dataset\n",
    "data = pd.read_csv(\"data/clean_dialect_dataset.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confirmed-mineral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dialect</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.175358e+18</td>\n",
       "      <td>IQ</td>\n",
       "      <td>لكن بالنهايه  ينتفض  يغير</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.175416e+18</td>\n",
       "      <td>IQ</td>\n",
       "      <td>يعني هذا محسوب علي البشر  حيونه ووحشيه  وتطلبو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.175450e+18</td>\n",
       "      <td>IQ</td>\n",
       "      <td>مبين من كلامه خليجي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.175471e+18</td>\n",
       "      <td>IQ</td>\n",
       "      <td>يسلملي مرورك وروحك الحلوه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.175497e+18</td>\n",
       "      <td>IQ</td>\n",
       "      <td>وين هل الغيبه  اخ محمد</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id dialect                                               text\n",
       "0  1.175358e+18      IQ                          لكن بالنهايه  ينتفض  يغير\n",
       "1  1.175416e+18      IQ  يعني هذا محسوب علي البشر  حيونه ووحشيه  وتطلبو...\n",
       "2  1.175450e+18      IQ                                مبين من كلامه خليجي\n",
       "3  1.175471e+18      IQ                          يسلملي مرورك وروحك الحلوه\n",
       "4  1.175497e+18      IQ                             وين هل الغيبه  اخ محمد"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3899767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458187, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bizarre-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encoding the dialect\n",
    "encoder = LabelEncoder()\n",
    "data[\"dialect_transformed\"] = encoder.fit_transform(data[\"dialect\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "floral-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will save for future use in the server\n",
    "output = open('model/dialect_encoder.pkl', 'wb')\n",
    "pickle.dump(encoder, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9dd8e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(311566, 4) (54983, 4) (91638, 4)\n"
     ]
    }
   ],
   "source": [
    "#Splitting data into three datasets\n",
    "training_data, testing_data = train_test_split(data, train_size=0.8,\n",
    "                                               stratify=data.loc[:,\"dialect_transformed\"],\n",
    "                                               random_state=77)\n",
    "\n",
    "training_data, validation_data = train_test_split(training_data, train_size=0.85,\n",
    "                                                  stratify=training_data.loc[:,\"dialect_transformed\"],\n",
    "                                                  random_state=77)\n",
    "\n",
    "print(training_data.shape,validation_data.shape, testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc03f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the Arabic stopwords ->> Source SpaCy Library Stop words\n",
    "from data.stopwords import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b6086c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets' create the TF-IDF vectorizer of the text and using n-grams (1-4)\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb', stop_words=stopwords,\n",
    "                             ngram_range=(1,4),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-aspect",
   "metadata": {},
   "source": [
    "### First we we'll try a baseline model. Let's try the Logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "occupied-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=7, penalty= \"l1\",\n",
    "                          verbose=True, n_jobs=-1, solver='liblinear')\n",
    "pipe1=make_pipeline(vectorizer,clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "distributed-ceiling",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 4),\n",
       "                                 stop_words=['،', 'ء', 'ءَ', 'آ', 'آب', 'آذار',\n",
       "                                             'آض', 'آل', 'آمينَ', 'آناء',\n",
       "                                             'آنفا', 'آه', 'آهاً', 'آهٍ', 'آهِ',\n",
       "                                             'أ', 'أبدا', 'أبريل', 'أبو', 'أبٌ',\n",
       "                                             'أجل', 'أجمع', 'أحد', 'أخبر',\n",
       "                                             'أخذ', 'أخو', 'أخٌ', 'أربع',\n",
       "                                             'أربعاء', 'أربعة', ...])),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(n_jobs=-1, penalty='l1', random_state=7,\n",
       "                                    solver='liblinear', verbose=True))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe1.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "modern-batman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.40      0.42      3156\n",
      "           1       0.38      0.28      0.32      3155\n",
      "           2       0.64      0.48      0.54      1942\n",
      "           3       0.66      0.86      0.75      6916\n",
      "           4       0.64      0.51      0.57      1860\n",
      "           5       0.43      0.28      0.34      3351\n",
      "           6       0.44      0.63      0.52      5053\n",
      "           7       0.61      0.66      0.64      3314\n",
      "           8       0.59      0.70      0.64      4380\n",
      "           9       0.76      0.55      0.64      1385\n",
      "          10       0.46      0.34      0.39      2294\n",
      "          11       0.45      0.58      0.51      5249\n",
      "          12       0.48      0.46      0.47      3728\n",
      "          13       0.39      0.43      0.41      3220\n",
      "          14       0.71      0.52      0.60      1732\n",
      "          15       0.55      0.25      0.35      1949\n",
      "          16       0.76      0.36      0.49      1108\n",
      "          17       0.55      0.11      0.19      1191\n",
      "\n",
      "    accuracy                           0.53     54983\n",
      "   macro avg       0.55      0.47      0.49     54983\n",
      "weighted avg       0.53      0.53      0.51     54983\n",
      "\n",
      "Score: 0.5279813760616918\n"
     ]
    }
   ],
   "source": [
    "pred=pipe1.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-navigator",
   "metadata": {},
   "source": [
    "The score is very bad. Let's try a quick hyperparameter tuning to see if we can make some improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:1355: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    }
   ],
   "source": [
    "# define search space\n",
    "space = {\"logisticregression__C\":[1.0,2.0,3.0, 5.0, 6.0, 7.0]}\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(pipe1, space, scoring='accuracy')\n",
    "\n",
    "# execute search\n",
    "result = search.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize result\n",
    "print(f'Best Score: {result.best_score_}')\n",
    "print(f'Best Hyperparameters: {result.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the accuracy improvement on validation dataset\n",
    "pred=search.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-forge",
   "metadata": {},
   "source": [
    "There is a slight improvement. Let's try what will happen if we used the Count Vectorizer instead of Tf-idf to avoid decreasing the weight of the most common words in most of the dialects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try the count vectorizer of the text and using n-grams (1-4)\n",
    "count_vectorizer = CountVectorizer(analyzer='char_wb', stop_words=stopwords,\n",
    "                                   ngram_range=(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=7,penalty= \"l1\", verbose=True, solver='liblinear')\n",
    "pipe1=make_pipeline(count_vectorizer,clf1)\n",
    "pipe1.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pipe1.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-nelson",
   "metadata": {},
   "source": [
    "Changing the Vectorization method didn't affect the results, so let's try another model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-spain",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "animated-simple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 4),\n",
       "                                 stop_words=['،', 'ء', 'ءَ', 'آ', 'آب', 'آذار',\n",
       "                                             'آض', 'آل', 'آمينَ', 'آناء',\n",
       "                                             'آنفا', 'آه', 'آهاً', 'آهٍ', 'آهِ',\n",
       "                                             'أ', 'أبدا', 'أبريل', 'أبو', 'أبٌ',\n",
       "                                             'أجل', 'أجمع', 'أحد', 'أخبر',\n",
       "                                             'أخذ', 'أخو', 'أخٌ', 'أربع',\n",
       "                                             'أربعاء', 'أربعة', ...])),\n",
       "                ('complementnb', ComplementNB())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2=ComplementNB(fit_prior=True)\n",
    "pipe2=make_pipeline(vectorizer,clf2)\n",
    "pipe2.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dying-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.28      0.34      3156\n",
      "           1       0.42      0.22      0.29      3155\n",
      "           2       0.54      0.42      0.47      1942\n",
      "           3       0.51      0.91      0.65      6916\n",
      "           4       0.63      0.45      0.53      1860\n",
      "           5       0.49      0.15      0.23      3351\n",
      "           6       0.38      0.63      0.48      5053\n",
      "           7       0.54      0.64      0.59      3314\n",
      "           8       0.59      0.65      0.62      4380\n",
      "           9       0.66      0.55      0.60      1385\n",
      "          10       0.55      0.20      0.30      2294\n",
      "          11       0.39      0.56      0.46      5249\n",
      "          12       0.41      0.47      0.44      3728\n",
      "          13       0.42      0.32      0.36      3220\n",
      "          14       0.67      0.33      0.44      1732\n",
      "          15       0.56      0.12      0.20      1949\n",
      "          16       0.66      0.30      0.42      1108\n",
      "          17       0.57      0.09      0.15      1191\n",
      "\n",
      "    accuracy                           0.48     54983\n",
      "   macro avg       0.52      0.40      0.42     54983\n",
      "weighted avg       0.49      0.48      0.45     54983\n",
      "\n",
      "Score: 0.47814779113544187\n"
     ]
    }
   ],
   "source": [
    "pred=pipe2.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "space = {\"complementnb__alpha\":[0.2, 0.25, 0.3, 0.35, 0.40, 0.45]}\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "\n",
    "# define search\n",
    "search2 = GridSearchCV(pipe2, space, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# execute search\n",
    "result2 = search2.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize result\n",
    "print(f'Best Score: {result2.best_score_}')\n",
    "print(f'Best Hyperparameters: {result2.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's check the accuracy improvement on validation dataset\n",
    "pred=search2.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-grant",
   "metadata": {},
   "source": [
    "### Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81e63645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-9)]: Using backend ThreadingBackend with 28 concurrent workers.\n",
      "[Parallel(n_jobs=-9)]: Done 100 out of 100 | elapsed:  9.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 4),\n",
       "                                 stop_words=['،', 'ء', 'ءَ', 'آ', 'آب', 'آذار',\n",
       "                                             'آض', 'آل', 'آمينَ', 'آناء',\n",
       "                                             'آنفا', 'آه', 'آهاً', 'آهٍ', 'آهِ',\n",
       "                                             'أ', 'أبدا', 'أبريل', 'أبو', 'أبٌ',\n",
       "                                             'أجل', 'أجمع', 'أحد', 'أخبر',\n",
       "                                             'أخذ', 'أخو', 'أخٌ', 'أربع',\n",
       "                                             'أربعاء', 'أربعة', ...])),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(n_jobs=-9, random_state=77,\n",
       "                                        verbose=True))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3=RandomForestClassifier(verbose=True, n_jobs=-9, random_state=77)\n",
    "pipe3=make_pipeline(vectorizer,clf3)\n",
    "pipe3.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac965cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=28)]: Using backend ThreadingBackend with 28 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.14      0.19      3156\n",
      "           1       0.31      0.17      0.22      3155\n",
      "           2       0.64      0.24      0.35      1942\n",
      "           3       0.40      0.87      0.55      6916\n",
      "           4       0.63      0.22      0.32      1860\n",
      "           5       0.31      0.12      0.17      3351\n",
      "           6       0.28      0.59      0.38      5053\n",
      "           7       0.54      0.47      0.50      3314\n",
      "           8       0.42      0.47      0.45      4380\n",
      "           9       0.79      0.27      0.40      1385\n",
      "          10       0.37      0.10      0.16      2294\n",
      "          11       0.30      0.51      0.38      5249\n",
      "          12       0.39      0.31      0.34      3728\n",
      "          13       0.36      0.19      0.25      3220\n",
      "          14       0.77      0.18      0.29      1732\n",
      "          15       0.66      0.06      0.12      1949\n",
      "          16       0.76      0.12      0.21      1108\n",
      "          17       0.62      0.04      0.07      1191\n",
      "\n",
      "    accuracy                           0.37     54983\n",
      "   macro avg       0.49      0.28      0.30     54983\n",
      "weighted avg       0.43      0.37      0.34     54983\n",
      "\n",
      "Score: 0.37364276230834986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=28)]: Done 100 out of 100 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "pred=pipe3.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search space\n",
    "space = {\"randomforestclassifier__n_estimators\":[100, 150, 200, 250]}\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "\n",
    "# define search\n",
    "search3 = GridSearchCV(pipe3, space, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# execute search\n",
    "result3 = search3.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-painting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize result\n",
    "print(f'Best Score: {result3.best_score_}')\n",
    "print(f'Best Hyperparameters: {result3.best_params_}')\n",
    "\n",
    "#Let's check the accuracy improvement on test dataset\n",
    "pred=search3.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-butler",
   "metadata": {},
   "source": [
    "### Voting Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-perry",
   "metadata": {},
   "source": [
    "Now Let's merge all the models with their best parameters, from the hyperparameter tuning, in a voting classifier to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedicated-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:1357: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 36.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 128 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=-1)]: Done 250 out of 250 | elapsed: 25.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', ngram_range=(1, 4),\n",
       "                                 stop_words=['،', 'ء', 'ءَ', 'آ', 'آب', 'آذار',\n",
       "                                             'آض', 'آل', 'آمينَ', 'آناء',\n",
       "                                             'آنفا', 'آه', 'آهاً', 'آهٍ', 'آهِ',\n",
       "                                             'أ', 'أبدا', 'أبريل', 'أبو', 'أبٌ',\n",
       "                                             'أجل', 'أجمع', 'أحد', 'أخبر',\n",
       "                                             'أخذ', 'أخو', 'أخٌ', 'أربع',\n",
       "                                             'أربعاء', 'أربعة', ...])),\n",
       "                ('votingclassifier',\n",
       "                 VotingClassifier(estimators=[('lr',\n",
       "                                               LogisticRegression(C=7.0,\n",
       "                                                                  n_jobs=-1,\n",
       "                                                                  penalty='l1',\n",
       "                                                                  random_state=7,\n",
       "                                                                  solver='liblinear',\n",
       "                                                                  verbose=True)),\n",
       "                                              ('nb', ComplementNB(alpha=0.25)),\n",
       "                                              ('rf',\n",
       "                                               RandomForestClassifier(n_estimators=250,\n",
       "                                                                      n_jobs=-1,\n",
       "                                                                      random_state=77,\n",
       "                                                                      verbose=True))],\n",
       "                                  voting='soft'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = LogisticRegression(random_state=7,penalty= \"l1\",C =7.0,verbose=True,n_jobs=-1,solver='liblinear')\n",
    "\n",
    "clf2=ComplementNB(fit_prior=True, alpha=0.25)\n",
    "\n",
    "clf3 = RandomForestClassifier(n_estimators=250, verbose=True, n_jobs=-1, random_state=77)\n",
    "\n",
    "clf = VotingClassifier(estimators=[('lr', clf1), ('nb', clf2),('rf', clf3)], voting='soft')\n",
    "pipe=make_pipeline(vectorizer,clf)\n",
    "pipe.fit(training_data.text,training_data.dialect_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bored-member",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 128 tasks      | elapsed:    2.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.40      0.43      3156\n",
      "           1       0.39      0.33      0.36      3155\n",
      "           2       0.66      0.52      0.58      1942\n",
      "           3       0.69      0.88      0.77      6916\n",
      "           4       0.66      0.54      0.59      1860\n",
      "           5       0.42      0.31      0.36      3351\n",
      "           6       0.47      0.62      0.54      5053\n",
      "           7       0.64      0.67      0.66      3314\n",
      "           8       0.65      0.73      0.68      4380\n",
      "           9       0.78      0.57      0.66      1385\n",
      "          10       0.46      0.37      0.41      2294\n",
      "          11       0.47      0.59      0.52      5249\n",
      "          12       0.49      0.49      0.49      3728\n",
      "          13       0.44      0.44      0.44      3220\n",
      "          14       0.75      0.54      0.63      1732\n",
      "          15       0.52      0.31      0.38      1949\n",
      "          16       0.74      0.45      0.56      1108\n",
      "          17       0.44      0.19      0.27      1191\n",
      "\n",
      "    accuracy                           0.55     54983\n",
      "   macro avg       0.56      0.50      0.52     54983\n",
      "weighted avg       0.55      0.55      0.54     54983\n",
      "\n",
      "Score: 0.5507884255133405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 250 out of 250 | elapsed:    3.6s finished\n"
     ]
    }
   ],
   "source": [
    "#Score on Validation dataset\n",
    "pred=pipe.predict(validation_data.text)\n",
    "print(classification_report(validation_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(validation_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "present-capacity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 128 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=36)]: Done 250 out of 250 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.42      0.43      5259\n",
      "           1       0.40      0.33      0.36      5259\n",
      "           2       0.67      0.53      0.60      3237\n",
      "           3       0.69      0.87      0.77     11527\n",
      "           4       0.68      0.54      0.60      3099\n",
      "           5       0.44      0.34      0.38      5584\n",
      "           6       0.47      0.62      0.54      8422\n",
      "           7       0.65      0.69      0.67      5524\n",
      "           8       0.66      0.72      0.69      7300\n",
      "           9       0.78      0.60      0.68      2308\n",
      "          10       0.47      0.38      0.42      3823\n",
      "          11       0.47      0.59      0.53      8749\n",
      "          12       0.48      0.50      0.49      6214\n",
      "          13       0.44      0.44      0.44      5366\n",
      "          14       0.75      0.57      0.64      2887\n",
      "          15       0.50      0.30      0.38      3248\n",
      "          16       0.73      0.44      0.55      1847\n",
      "          17       0.43      0.20      0.27      1985\n",
      "\n",
      "    accuracy                           0.55     91638\n",
      "   macro avg       0.57      0.50      0.52     91638\n",
      "weighted avg       0.55      0.55      0.55     91638\n",
      "\n",
      "Score: 0.5547807678037495\n"
     ]
    }
   ],
   "source": [
    "#Final results on the testing dataset\n",
    "pred=pipe.predict(testing_data.text)\n",
    "print(classification_report(testing_data.dialect_transformed,pred))\n",
    "print('Score:',accuracy_score(testing_data.dialect_transformed,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "competitive-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final model to disk\n",
    "filename = 'model/finalized_model.sav'\n",
    "pickle.dump(pipe, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338dccf",
   "metadata": {},
   "source": [
    "It seems that the Voting Classifier with tuned hyper-parameters is the best one till now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
